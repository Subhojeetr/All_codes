{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from contractions import contractions_dict\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "#TAG_MAP=[\".\",\",\",\"-LRB-\",\"-RRB-\",\"``\",\"\\\"\\\"\",\"''\",\",\",\"$\",\"#\",\"AFX\",\"CC\",\"CD\",\"DT\",\"EX\",\"FW\",\"HYPH\",\"IN\",\"JJ\",\"JJR\",\"JJS\",\"LS\",\"MD\",\"NIL\",\"NN\",\"NNP\",\"NNPS\",\"NNS\",\"PDT\",\"POS\",\"PRP\",\"PRP$\",\"RB\",\"RBR\",\"RBS\",\"RP\",\"SP\",\"SYM\",\"TO\",\"UH\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\",\"WDT\",\"WP\",\"WP$\",\"WRB\",\"ADD\",\"NFP\",\"GW\",\"XX\",\"BES\",\"HVS\",\"_SP\",]\n",
    "TAG_MAP=[\"AFX\",\"CC\",\"CD\",\"DT\",\"EX\",\"FW\",\"HYPH\",\"IN\",\"JJ\",\"JJR\",\"JJS\",\"LS\",\"MD\",\"NIL\",\"NN\",\"NNP\",\"NNPS\",\"NNS\",\"PDT\",\"POS\",\"PRP\",\"PRP$\",\"RB\",\"RBR\",\"RBS\",\"RP\",\"SP\",\"SYM\",\"TO\",\"UH\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\",\"WDT\",\"WP\",\"WP$\",\"WRB\",\"ADD\",\"NFP\",\"GW\",\"XX\",\"BES\",\"HVS\",\"_SP\",]\n",
    "TAG_MAP_LOWER=[tag.lower() for tag in TAG_MAP]\n",
    "class DataPreProcessing():\n",
    "    \"\"\"This class will take a list of documents\n",
    "    as a parameter and return list of processed\n",
    "    data.\"\"\"\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    noisy_pos_tags = ['-PRON-','-PRON-']\n",
    "    \n",
    "\n",
    "    def __init__(self,list_of_doc,lemmatize=True,expand_contraction=True,remove_special_characters=True):\n",
    "        self()\n",
    "        self.__list_of_doc=list_of_doc\n",
    "        self._remove_special_characters=remove_special_characters\n",
    "        self._lemmatize=lemmatize\n",
    "        self._expand_contraction=expand_contraction        \n",
    "    \n",
    "\n",
    "    def normalized_corpus(self):\n",
    "        return self.processData()\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def Noise(token):     \n",
    "        is_noise = False\n",
    "        if token.is_stop == True:\n",
    "            is_noise = True\n",
    "        elif token.pos_ in DataPreProcessing.noisy_pos_tags:\n",
    "            is_noise = True \n",
    "        elif token.string.strip() in DataPreProcessing.noisy_pos_tags:\n",
    "            is_noise = True\n",
    "        return is_noise\n",
    "\n",
    "    def __call__(self):\n",
    "        add_List=['']\n",
    "        remove_List=['not','no']\n",
    "        for w in add_List:\n",
    "            DataPreProcessing.nlp.vocab[w].is_stop = True\n",
    "\n",
    "        for w in remove_List:\n",
    "            DataPreProcessing.nlp.vocab[w].is_stop = False\n",
    "\n",
    "    # # Expanding Contractions\n",
    "    @staticmethod\n",
    "    def expand_contractions(text,regex,contraction_mapping):\n",
    "        match_list=regex.findall(text)\n",
    "        if len(match_list)>0:\n",
    "            for word in match_list:\n",
    "                text=re.sub(word,contraction_mapping.get(word),text)\n",
    "        return text\n",
    "\n",
    "    # # Removing Special Characters\n",
    "    @staticmethod\n",
    "    def remove_special_characters(text):\n",
    "        text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n",
    "        return text\n",
    "    \n",
    "    def processData(self):\n",
    "        corpus=[]\n",
    "        contraction_mapping={k.lower(): v.lower() for k, v in contractions_dict.items()}\n",
    "        ####correcting expansions of some keys\n",
    "        contraction_mapping[\"they'd've\"]=\"they would have\"\n",
    "        contraction_mapping[\"couldn't've\"]=\"could not have\"\n",
    "        contraction_mapping[\"y’all’d\"]=\"you all would\"\n",
    "        contraction_mapping[\"we’ll’ve\"]=\"we will have\"\n",
    "        contraction_mapping[\"i'd've\"]=\"I would have\"\n",
    "        contraction_mapping[\"wouldn’t’ve\"]=\"would not have\"\n",
    "        contraction_mapping[\"mustn’t’ve\"]=\"must not have\"\n",
    "        contraction_mapping[\"won’t’ve\"]=\"will not have\"\n",
    "        contraction_mapping[\"he'd've\"]=\"he would have\"\n",
    "        contraction_mapping[\"mightn't've\"]=\"might not have\"\n",
    "        contraction_mapping[\"y’all’re\"]=\"you all are\"\n",
    "        contraction_mapping[\"oughtn’t’ve\"]=\"ought not have\"\n",
    "        contraction_mapping[\"it’ll’ve\"]=\"it will have\"\n",
    "        contraction_mapping[\"who’ll’ve\"]=\"who will have\"\n",
    "        contraction_mapping[\"hadn’t’ve\"]=\"had not have\"\n",
    "        contraction_mapping[\"she’d’ve\"]=\"she would have\"\n",
    "        contraction_mapping[\"oughtn't've\"]=\"ought not have\"\n",
    "        contraction_mapping[\"there'd've\"]=\"there would have\"\n",
    "        contraction_mapping[\"y’all’ve\"]=\"you all have\"\n",
    "        contraction_mapping[\"mightn’t’ve\"]=\"might not have\"\n",
    "        contraction_mapping[\"shouldn't've\"]=\"should not have\"\n",
    "        contraction_mapping[\"how'd'y\"]=\"how did you\"\n",
    "        contraction_mapping[\"i’ll’ve\"]=\"i will have\"\n",
    "        contraction_mapping[\"y'all've\"]=\"you all have\"\n",
    "        contraction_mapping[\"mustn’t’ve\"]=\"must not have\"\n",
    "        contraction_mapping[\"she’ll’ve\"]=\"she will have\"\n",
    "        contraction_mapping[\"they’ll’ve\"]=\"they will have\"\n",
    "        contraction_mapping[\"shouldn’t’ve\"]=\"should not have\"\n",
    "        contraction_mapping[\"oughtn’t’ve\"]=\"ought not have\"\n",
    "        contraction_mapping[\"shan't've\"]=\"shall not have\"\n",
    "        contraction_mapping[\"it’ll’ve\"]=\"it will have\"\n",
    "        contraction_mapping[\"shan’t’ve\"]=\"shall not have\"\n",
    "        contraction_mapping[\"who’ll’ve\"]=\"who will have\"\n",
    "        contraction_mapping[\"hadn’t’ve\"]=\"had not have\"\n",
    "        contraction_mapping[\"needn’t’ve\"]=\"need not have\"\n",
    "        contraction_mapping[\"mightn’t’ve\"]=\"might not have\"\n",
    "        contraction_mapping[\"shouldn't've\"]=\"should not have\"\n",
    "        contraction_mapping[\"how'd'y\"]=\"how did you\"\n",
    "        contraction_mapping[\"i’d’ve\"]=\"i would have\"\n",
    "        contraction_mapping[\"we'll've\"]=\"we will have\"\n",
    "        contraction_mapping[\"he'll've\"]=\"he will have\"\n",
    "        contraction_mapping[\"wouldn't've\"]=\"would not have\"\n",
    "        contraction_mapping[\"we’d’ve\"]=\"we would have\"\n",
    "        contraction_mapping[\"can't've\"]=\"cannot have\"\n",
    "        contraction_mapping[\"couldn’t’ve\"]=\"could not have\"\n",
    "        contraction_mapping[\"i'll've\"]=\"i will have\"\n",
    "        contraction_mapping[\"what’ll’ve\"]=\"what will have\"\n",
    "        contraction_mapping[\"y’all’d’ve\"]=\"you all would have\"\n",
    "        contraction_mapping[\"y'all're\"]=\"you all are\"\n",
    "        contraction_mapping[\"there’d’ve\"]=\"there would have\"\n",
    "        contraction_mapping[\"he’d’ve\"]=\"he would have\"\n",
    "        contraction_mapping[\"you'd've\"]=\"you would have\"\n",
    "        contraction_mapping[\"there’d’ve\"]=\"there would have\"\n",
    "        contraction_mapping[\"he’ll’ve\"]=\"he will have\"\n",
    "        contraction_mapping[\"will've\"]=\"will have\"\n",
    "        contraction_mapping[\"cannot’ve\"]=\"cannot have\"\n",
    "        contraction_mapping[\"you will've\"]=\"you will have\"\n",
    "        contraction_mapping[\"he will’ve\"]=\"he will have\"\n",
    "        contraction_mapping[\"will not've\"]=\"will not have\"\n",
    "        contraction_mapping[\"he would’ve\"]=\"he would have\"\n",
    "        contraction_mapping[\"all’ve\"]=\"all have\"\n",
    "        \"\"\"\n",
    "        print(\"***********************************************************************************************************************************\")\n",
    "        with open(\"contraction.json\",\"w\") as f1:\n",
    "            json.dump(contraction_mapping,f1,ensure_ascii=True)\n",
    "\n",
    "        with open(\"contraction.json\",\"r\") as f1:\n",
    "            abc=json.loads(f1.read())\n",
    "            print(abc.items())\n",
    "        print(\"***********************************************************************************************************************************\")\n",
    "        \"\"\"\n",
    "        regex = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "        for doc in self.__list_of_doc:\n",
    "            if self._expand_contraction:\n",
    "                doc=self.expand_contractions(doc.strip().lower(),regex,contraction_mapping)\n",
    "\n",
    "            if self._remove_special_characters:\n",
    "                doc=self.remove_special_characters(doc)\n",
    "                \n",
    "            document=DataPreProcessing.nlp(doc.lower())\n",
    "            if not self._lemmatize:\n",
    "                #cleaned_list = \" \".join([(token.string.strip(),token.tag_) for token in document if not self.Noise(token) and len(token.string.strip())>0])\n",
    "                corpus.append(cleaned_list)\n",
    "            else:\n",
    "                #cleaned_list = \" \".join([(token.lemma_.strip(),token.tag_) for token in document if not self.Noise(token) and len(token.string.strip())>0])\n",
    "                cleaned_list = [[token.lemma_.strip(),token.tag_] for token in document if not self.Noise(token) and len(token.string.strip())>0]\n",
    "                alst=[]\n",
    "                #cleaned_list_b=\" \".join(alst.append(\" \".join(tup)) for tup in cleaned_list)\n",
    "                for tup in cleaned_list:\n",
    "                    alst.append(\" \".join(tup))\n",
    "                    #print(alst)\n",
    "                          \n",
    "                corpus.append(alst)\n",
    "                \n",
    "        corpus_mod=[\" \".join(lst) for lst in corpus]\n",
    "        print(corpus)\n",
    "        return corpus_mod  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\subhojeet.rudra\\appdata\\local\\continuum\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def getVocab(corpus):\n",
    "    vocab=[]\n",
    "    for text in corpus:\n",
    "        bigrm = nltk.bigrams(text.split())\n",
    "        b=[\" \".join(tup) for tup in bigrm if tup[0] not in TAG_MAP]\n",
    "        \n",
    "        vocab.extend(b)\n",
    "    return set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question(corpus):\n",
    "    #vocabulary=getVocab(corpus)\n",
    "    #tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),sublinear_tf=True,vocabulary=vocabulary)\n",
    "    tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(2,3),sublinear_tf=True)\n",
    "    tfidf_matrix_train = tv.fit_transform(corpus)\n",
    "    #vocab=tv.get_feature_names()\n",
    "    #tfidf_matrix_train=tfidf_matrix_train.toarray()\n",
    "    #print(tv.vocabulary_)\n",
    "    #print(type(vocab))\n",
    "    #print(tv)\n",
    "    #df=pd.DataFrame(tfidf_matrix_train,columns=vocab)\n",
    "    #print(df)\n",
    "    #print(tfidf_matrix_train.shape)\n",
    "    return tv, tfidf_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_DB(file,text):\n",
    "    import os,json\n",
    "    from lockfile import LockFile\n",
    "    if os.path.isfile(file):\n",
    "        lock = LockFile(file)\n",
    "        with lock:\n",
    "            with open(file,\"r+\") as f1:\n",
    "                aDict=json.loads(f1.read())\n",
    "                f1.seek(0)\n",
    "                aDict['questions'].append(text)\n",
    "                json.dump(aDict,f1,ensure_ascii=True)\n",
    "                f1.truncate()\n",
    "    \n",
    "    else:\n",
    "        aDict=dict()\n",
    "        aDict['questions']=[]\n",
    "        aDict['questions'].append(text)\n",
    "        with open(file,\"w\") as f1:\n",
    "            json.dump(aDict,f1,ensure_ascii=True)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "18888\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c27c71c1e38d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mcorpus_mod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mtv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtfidf_matrix_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_mod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mquestion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rudra:\"\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "def reply(test,tv,tfidf_matrix_train):\n",
    "    #print(tv)\n",
    "    #print(\"b\")\n",
    "    test=(test,)\n",
    "    tfidf_matrix_test = tv.transform(test)\n",
    "    cosine = cosine_similarity(tfidf_matrix_test, tfidf_matrix_train)\n",
    "    print(cosine)\n",
    "    #print(tfidf_matrix_test, tfidf_matrix_train)\n",
    "\n",
    "\n",
    "    minimum_score=0.3\n",
    "    #cosine = np.delete(cosine, 0)  #not required\n",
    "    #print(cosine)\n",
    "    maxa = cosine.max()\n",
    "    #print(maxa)\n",
    "    response_index=-99999\n",
    "    if (maxa >= minimum_score):\n",
    "        #print(\"hello\")\n",
    "        #new_max = maxa - 0.01\n",
    "        alist = np.where(cosine > minimum_score)[1]\n",
    "        #alist = np.where(cosine > new_max)\n",
    "        # print (\"number of responses with 0.01 from max = \" + str(list[0].size))\n",
    "        #response_index = random.choice(alist[0])\n",
    "        \n",
    "        #print(response_index)\n",
    "        for index in alist:\n",
    "            print(\"**************\")\n",
    "            print(index)\n",
    "            print(target[index])\n",
    "            print(data[index])\n",
    "            print(\"**************\")\n",
    "\n",
    "        blist=np.where(cosine==maxa)[1]\n",
    "        print(\"******MAX b********\")\n",
    "        print(blist)\n",
    "        print(\"******MAX b********\")\n",
    "        final_list=[]\n",
    "        for index in alist:\n",
    "            final_list.append(target[index])\n",
    "            \n",
    "        print(\"replied\")\n",
    "        return final_list\n",
    "def main(data):\n",
    "    from os import getpid\n",
    "    d=DataPreProcessing(data)\n",
    "    corpus=d.normalized_corpus()\n",
    "    #print(corpus)\n",
    "    return corpus\n",
    "if __name__==\"__main__\":\n",
    "    data=['how to resolve error for Server Load','How to resolve error for No file found','job abended or failed  for script','how to execute a script','efgh ',\\\n",
    "      'Hi']\n",
    "    target=['run top to idntify the process.kill it ','specify the correct directory','check the logs','execute a sh/source','abcd','Hey Let me know how could I help you']\n",
    "\n",
    "    \n",
    "    q=[]\n",
    "    a=[]\n",
    "    print(len(a))\n",
    "    with open('train.json', \"r\") as sentences_file:\n",
    "        reader = json.load(sentences_file)\n",
    "        for item in reader['data']:\n",
    "            if type(item)==dict:\n",
    "                for qas in item['paragraphs']:\n",
    "                    for question in qas['qas']:\n",
    "                        try:\n",
    "                            a.append(question['answers'][0]['text'])\n",
    "                            q.append(question['question'])\n",
    "                        \n",
    "                        except:\n",
    "                            pass\n",
    "                        break\n",
    "    q.extend(data)\n",
    "    a.extend(target)\n",
    "    print(len(a))\n",
    "    data=q\n",
    "    target=a\n",
    "\n",
    "    del a\n",
    "    del q\n",
    "    \n",
    "    \n",
    "    corpus_mod=main(data)\n",
    "    tv, tfidf_matrix_train=question(corpus_mod)\n",
    "    while True:\n",
    "        question=input(\"rudra:\" )      \n",
    "        ques=(question,)\n",
    "        ques=\" \".join(main(ques))\n",
    "        print(\"Mod question\")\n",
    "        print(ques)\n",
    "        response=reply(ques,tv,tfidf_matrix_train)\n",
    "        if response is None:\n",
    "            print(\"I am Sorry!! I am not aware of this\")\n",
    "            learning_DB(\"learning.json\",question)\n",
    "        else:\n",
    "            print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\subhojeet.rudra\\appdata\\local\\continuum\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.\n",
      "  _nan_object_mask = _nan_object_array != _nan_object_array\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=\"Incident IR53463667 is created because job MMP_GHG_040 failed or abended due to server overload\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Incident/NNP)\n",
      "  (ORGANIZATION IR53463667/NNP)\n",
      "  is/VBZ\n",
      "  created/VBN\n",
      "  because/IN\n",
      "  job/NN\n",
      "  (ORGANIZATION MMP_GHG_040/NNP)\n",
      "  failed/VBD\n",
      "  or/CC\n",
      "  abended/VBD\n",
      "  due/JJ\n",
      "  to/TO\n",
      "  server/VB\n",
      "  overload/NN)\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Incident', 'PERSON'), ('IR53463667', 'ORGANIZATION'), ('MMP_GHG_040', 'ORGANIZATION')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "doc = sentence\n",
    "# tokenize doc\n",
    "tokenized_doc = nltk.word_tokenize(doc)\n",
    " \n",
    "# tag sentences and use nltk's Named Entity Chunker\n",
    "tagged_sentences = nltk.pos_tag(tokenized_doc)\n",
    "ne_chunked_sents = nltk.ne_chunk(tagged_sentences)\n",
    " \n",
    "# extract all named entities\n",
    "named_entities = []\n",
    "for tagged_tree in ne_chunked_sents:\n",
    "    if hasattr(tagged_tree, 'label'):\n",
    "        entity_name = ' '.join(c[0] for c in tagged_tree.leaves()) #\n",
    "        entity_type = tagged_tree.label() # get NE category\n",
    "        named_entities.append((entity_name, entity_type))\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Linux', 'OS'), ('is', 'IR'), ('the', 'IR'), ('best', 'IR'), ('OS', 'IR')], [('Among', 'IR'), ('windows', 'OS'), ('and ', 'IR'), ('mac', 'OS'), ('ubuntu', 'OS'), ('is', 'IR'), ('better', 'IR')], [('Ubuntu', 'OS'), ('is', 'IR'), ('my', 'IR'), ('favourite', 'IR'), ('OS', 'IR')]]\n"
     ]
    }
   ],
   "source": [
    "data = [(['Linux', 'is', 'the', 'best', 'OS'], ['OS','IR','IR','IR','IR']),\n",
    "        (['Among', 'windows','and ', 'mac',  'ubuntu','is','better'], ['IR','OS','IR','OS','OS','IR','IR']),\n",
    "        #(['I','am','planning','to','buy','mac'],['IR','IR','IR','IR','IR','OS']),\n",
    "(['Ubuntu', 'is', 'my', 'favourite', 'OS'], ['OS','IR','IR','IR','IR'])]\n",
    "corpus = []\n",
    "for (doc, tags) in data:\n",
    "    doc_tag = []\n",
    "    for word, tag in zip(doc,tags):\n",
    "        doc_tag.append((word, tag))\n",
    "    corpus.append(doc_tag)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'word.word': 'Linux', 'word.nextword': 'is', 'BOS': True}, {'word.word': 'is', 'word.nextword': 'the', 'word.prevword': 'Linux'}, {'word.word': 'the', 'word.nextword': 'best', 'word.prevword': 'is'}, {'word.word': 'best', 'word.nextword': 'OS', 'word.prevword': 'the'}, {'word.word': 'OS', 'EOS': True, 'word.prevword': 'best'}], [{'word.word': 'Among', 'word.nextword': 'windows', 'BOS': True}, {'word.word': 'windows', 'word.nextword': 'and ', 'word.prevword': 'Among'}, {'word.word': 'and ', 'word.nextword': 'mac', 'word.prevword': 'windows'}, {'word.word': 'mac', 'word.nextword': 'ubuntu', 'word.prevword': 'and '}, {'word.word': 'ubuntu', 'word.nextword': 'is', 'word.prevword': 'mac'}, {'word.word': 'is', 'word.nextword': 'better', 'word.prevword': 'ubuntu'}, {'word.word': 'better', 'EOS': True, 'word.prevword': 'is'}], [{'word.word': 'Ubuntu', 'word.nextword': 'is', 'BOS': True}, {'word.word': 'is', 'word.nextword': 'my', 'word.prevword': 'Ubuntu'}, {'word.word': 'my', 'word.nextword': 'favourite', 'word.prevword': 'is'}, {'word.word': 'favourite', 'word.nextword': 'OS', 'word.prevword': 'my'}, {'word.word': 'OS', 'EOS': True, 'word.prevword': 'favourite'}]]\n"
     ]
    }
   ],
   "source": [
    "def doc2features(doc, i):\n",
    "    word = doc[i][0]\n",
    "    \n",
    "    # Features from current word\n",
    "    features={\n",
    "        'word.word': word,\n",
    "    }\n",
    "    # Features from previous word\n",
    "    if i > 0:\n",
    "        prevword = doc[i-1][0]\n",
    "        features['word.prevword'] = prevword\n",
    "    else:\n",
    "        features['BOS'] = True # Special \"Beginning of Sequence\" tag\n",
    "        \n",
    "    # Features from next word\n",
    "    if i < len(doc)-1:\n",
    "        nextword = doc[i+1][0]\n",
    "        features['word.nextword'] = nextword\n",
    "    else:\n",
    "        features['EOS'] = True # Special \"End of Sequence\" tag\n",
    "    return features\n",
    " \n",
    "def extract_features(doc):\n",
    "    return [doc2features(doc, i) for i in range(len(doc))]\n",
    " \n",
    "X = [extract_features(doc) for doc in corpus]\n",
    "print(X)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['OS', 'IR', 'IR', 'IR', 'IR'], ['IR', 'OS', 'IR', 'OS', 'OS', 'IR', 'IR'], ['OS', 'IR', 'IR', 'IR', 'IR']]\n"
     ]
    }
   ],
   "source": [
    "def get_labels(doc):\n",
    "    return [tag for (token,tag) in doc]\n",
    "y = [get_labels(doc) for doc in corpus]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=20,\n",
    "    all_possible_transitions=False,\n",
    ")\n",
    "crf.fit(X, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word.word': 'centos', 'EOS': True, 'BOS': True}\n",
      "['IR']\n"
     ]
    }
   ],
   "source": [
    "#test = [['CentOS', 'is', 'my', 'favourite', 'OS']]\n",
    "test = [['I','am','planning','to','buy','centos']]\n",
    "\n",
    "X_test = extract_features(test)\n",
    "X_test[0]['word.word']='centos'\n",
    "print(dict(X_test[0]))\n",
    "print(crf.predict_single(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
